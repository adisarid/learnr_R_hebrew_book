---
title: "Data Science עם R - Trees and (random)Forests"
author: "עדי שריד / adi@sarid-ins.co.il"
output: html_document
---
```{css, echo=FALSE}
p, h1, h2, h3, h4, ul, ol {
  direction: rtl;
}
```

ביחידה זו נלמד על שתי שיטות נוספות המשמשות לחיזוי ורגרסיה: עצים, ויערות אקראיים (trees and random forests).

תזכורת: באחת היחידות הקודמות התאמנו מודל רגרסיה לינארית למחיר יהלומים. כאשר פיצלנו את המודל לשני מודלים העובדים כל אחד על תחום של משתנה קטגורי, איכות החיזוי של המודל השתפרה.

הרעיון הכללי של עצים הוא דומה - לפצל את המרחב, כל פעם לפי משתנה אחר, עד שמגיעים ל"עלים" בהם ניתן החיזוי (כרגרסיה או סיווג).

עצים הם "נחמדים" כי הם מושכים ויזואלית ונוח לפרש אותם, אבל בדרך כלל הם לא נותנים תוצאות טובות, ומאוד רגישים לשינויים קלים (לדוגמה להוספה או החסרה של תצפיות). לכן, הכללות מקובלות לעצים הם כאלו המשכפלות את התהליך פעמים רבות (לדוגמה ל"יער אקראי", שבו גם נדון ביחידה זו). שכפול התהליך לעצים מרובים הופך את המודל ליותר רובוסטי ויותר מדויק, אבל קצת פחות ברור לפרשנות.

```{r fitting a tree to the diamonds data, warning=FALSE, message=FALSE}

library(tidyverse)

ggplot(diamonds, aes(y = price, x = carat)) + 
  facet_wrap(~ clarity) + 
  stat_smooth(method = "lm")

library(rpart)

diamond_price_tree <- rpart(formula = price ~ ., 
                            data = diamonds)

library(rpart.plot)
prp(diamond_price_tree)
diamond_price_tree
summary(diamond_price_tree)
```

פרמטרים שונים שולטים על עומק העץ. ככל שעץ עמוק יותר, כך אנחנו ניכנס למצבים של Over-fitting. הנה גידול עץ עמוק במיוחד (וכנראה לא מאוד מועיל).
```{r varying the complexity parameter}
diamond_price_tree_large <- rpart(formula = price ~ ., 
                                  data = diamonds,
                                  control = rpart.control(cp = 0.0005, xval = 10))
prp(diamond_price_tree_large)
#summary(diamond_price_tree_large)

```

הפרמטר cp שאותו שינינו כדי לשלוט על גודל העץ הוא פרמטר מורכבות העץ (complexity parameter). הוא שולט על אלגוריתם הגידול של העץ. כאשר הפרמטר נמוך, האלגוריתם נוטה לבצע יותר פיצולים, וכאשר הפרמטר גבוה, ישנם פחות פיצולים. למעשה הפרמטר מציב רף לפיצול, ורק אם פיצול משפר את החיזוי בערכו של הפרמטר, אז מתבצע הפיצול.

למעשה, אפשר גם לגדל עץ עמוק ואז לגזום אותו `prune`, כדי לייצר עץ קטן בחזרה. באופן מסויים זה מזכיר את האלגוריתם של step wise selection (במודל של רגרסיה) שאותו הזכרנו ביחידה קודמת. שני האלגוריתמים "הולכים אחורה" ומנסים להוריד משתנים.

איך עובד האלגוריתם של גידול וגיזום עצים?

## גידול וגיזום עצים

האלגוריתם מחלק את מרחב התצפיות $X$ למלבנים, בכל מלבן מתבצע ממוצע של ערכי התצפיות $y$, וזו התחזית הניתנת לתצפיות חדשות השייכות לאותו המלבן.

כלומר, האלגוריתם מנסה למזער את הגודל הבא:

\[\sum_{j=1}^J\sum_{i\in R_j}\left(y_i-\hat{y}_{R_j}\right)^2\]

כאשר $J$ הוא מספר המלבנים אליהם מחולק מרחב ה-$X$.

בכל רגע נתון באלגוריתם, נבחן הפיצול הטוב ביותר **כרגע**, כלומר, האלגוריתם מחפש את המשתנה ה-$X_j$ והערך הקריטי $s$, כך שיביא למינימום את הגודל:

\[\sum_{i: x_i\in R_1(j,s)}\left(y_i-\hat{y}_{R_1}\right)^2 + \sum_{i: x_i\in R_2(j,s)}\left(y_i-\hat{y}_{R_2}\right)^2\]

כאשר:

\[R_1(j,s) = \left\{X|X_j<s\right\} \text{ and } R_2(j,s) = \left\{X|X_j\geq s\right\}\]

במילים אחרות, מדובר באלגוריתם "חמדן" (הוא תמיד מחפש את הדבר הטוב ביותר כרגע, ולעיתים זה יכול להוביל לתוצאות לא טובות).

כדי לגזום עץ (להקטין את מידת הסיבוכיות שלו) ניתן להשתמש בפקודה `prune`.

```{r pruning a tree}

diamond_price_pruned <- prune(diamond_price_tree_large, cp = 0.05)

prp(diamond_price_pruned)

```

## שימוש ב-Cross validation לבחירת פרמטרים

כדי לבחון מה פרמטר ה-cp הרצוי, מומלץ להשתמש ב-cross validation.

מה עושה cross validation?

   * בחר ערך cp.
   * חלק את הנתונים ל-k חלקים (k-fold cross validation, $k=10$ היא בחירה נפוצה).
   * עבור $\frac{k-1}{k}$ מהנתונים בנה עץ באמצעות הפרמטר cp.
   * מקבלים $k$ ערכי שגיאה עבור הפרמטר cp - ובמילים אחרות את התפלגות השגיאה.
   * חזור על התהליך עבור ערכים שונים של cp.

האלגוריתם של `rpart` למעשה עושה את כל זה עבורנו.

```{r example for xvalidation}

# here is the cp table
diamond_price_tree_large$cptable

# the shortest way - use a predefined function to plot the xval cp errors
rpart::plotcp(diamond_price_tree_large)

```

במקרה זה, היות שיש לנו מדגם מאוד גדול של יהלומים, השגיאה אכן קטנה כאשר הפרמטר קטן, ואנחנו עוד לא בתחום של overfitting. במקרים אחרים ייתכן שנראה גרף שאינו מונוטוני, כלומר כאשר נקטין את cp, בשלב מסוים נקבל מצב שבו השגיאה גדלה.

כמו כן, ניתן לראות שהתפוקה השולית של הקטנת ה-cp פוחתת.

עד כה, דנו בעץ רגרסיה - עץ המספק חיזוי לערכים רציפים. ישנם עצי סיווג הרלוונטיים במקרים בהם נדרש לסווג תצפיות לערכים בדידים (classification), בדומה לאלגוריתמים אחרים שדנו בהם ביחידות קודמות (knn, רגרסיה לוגיסטית, lda, ו-qda). 

במקרה של עצי סיווג, לא משתמשים בשגיאת RSS אלא במדד אחר הנקרא Gini index.

\[G = \sum_{k=1}^K\hat{p}_{mk}(1-\hat{p}_{mk})\]

כאשר $\hat{p}_{mk}$ הוא הפרופורציה של תצפיות במרחב ה-$m$, עם סיווג $k$. מדד זה נמוך ככל שערכי $\hat{p}_{mk}$ הם יותר קיצוניים (קרובים ל-0 או ל-1), קרי העלים של העץ "טהורים".

```{r plot p time 1-p}

ggplot(tibble(p = seq(0, 1, 0.01)), aes(x = p, y = p*(1-p))) + 
  geom_line() + 
  ylab("G = p*(1-p)") +
  ggtitle("Illustration: Gini index will be minimized when p=1 or p=0")

```


בחלק מהאלגוריתמים נעשה שימוש במדד מקביל למדד Gini: מדד האנטרופיה.

\[D = -\sum_{k=1}^K{\hat{p}_{mk}\log\hat{p}_{mk}}\]


### תרגיל

בתרגיל הבא נשתמש בעצי החלטה כדי לחזות את הסבירות לנטישה של לקוח.

   1. קראו את הקובץ WA_Fn-UseC_-Telco-Customer-Churn.csv.
   2. בנו מודל עץ לחיזוי הנטישה, השתמשו ב-cp קטן וב-cp גדול.
   3. כעת ציירו את שני העצים, האם אתם מצליחים להפיק תובנות כלשהן מהעצים?
   4. הציגו את שגיאת ה-cross validation כפונקציה של cp. מה ערך cp שלדעתכם נכון לבחור?
   5. כעת השתמשו בערך cp שקיבלתם בסעיף הקודם. חלקו את הנתונים ל-train/test והשתמשו בנתוני ה-test כדי לייצר תרשים ROC (באפשרותכם להיעזר ביחידה שעסקה ברגרסיה לוגיסטית על מנת להיזכר בקוד הרלוונטי).
   6. התאימו מודל רגרסיה לוגיסטית מתחרה, ובצעו השוואה של לתוצאותיו למול תוצאות העץ (ציירו את שני ה-ROC עבור האלגוריתמים שבחרתם). האם יש אלגוריתם שניתן לומר שביצועיו טובים יותר?

```{r churn using rpart, include = FALSE}

# q1
telco_churn <- read_csv("data-files/WA_Fn-UseC_-Telco-Customer-Churn.csv") %>%
  select(-customerID)

# q2
telco_churn_tree <- rpart(data = telco_churn,
                          formula = Churn ~ .,
                          control = rpart.control(cp = 0.001))
# q2+q3
library(rpart.plot)
prp(telco_churn_tree)
telco_churn_short <- prune(telco_churn_tree, cp = 0.01)
prp(telco_churn_short)

# q4
printcp(telco_churn_tree)
plotcp(telco_churn_tree)

# q5
telco_churn <- telco_churn %>%
  mutate(is_train = runif(nrow(telco_churn)) < 0.8)

telco_churn_tree_train <- rpart(data = telco_churn %>% filter(is_train),
                                formula = Churn ~ . - is_train,
                                control = rpart.control(cp = 0.01))

telco_churn_deeptree_train <- rpart(data = telco_churn %>% filter(is_train),
                                formula = Churn ~ . - is_train,
                                control = rpart.control(cp = 0.000001))

# Competitive model using logistic regression
telco_churn_glm_train <- glm(formula = (Churn=="Yes") ~ . - is_train,
                             family = binomial,
                             data = telco_churn %>% filter(is_train))

telco_churn_roc <- telco_churn %>%
  mutate(probability_churn_tree = predict(telco_churn_tree_train, newdata = telco_churn)[, "Yes"]) %>%
  arrange(desc(probability_churn_tree)) %>%
  filter(!is_train) %>%
  mutate(churn_numeric = Churn == "Yes") %>%
  mutate(tpr=cumsum(churn_numeric)/sum(churn_numeric),
         fpr=cumsum(!churn_numeric)/sum(!churn_numeric)) %>%
  mutate(model = "tree_cp0.01")

telco_churn_roc_deep <- telco_churn %>%
  mutate(probability_churn_deeptree = 
           predict(telco_churn_deeptree_train, newdata = telco_churn)[, "Yes"]) %>%
  filter(!is_train) %>%
  mutate(churn_numeric = Churn == "Yes") %>%
  arrange(desc(probability_churn_deeptree)) %>%
  mutate(tpr=cumsum(churn_numeric)/sum(churn_numeric),
         fpr=cumsum(!churn_numeric)/sum(!churn_numeric)) %>%
  mutate(model = "tree_cp0.000001")

telco_churn_roc_glm <- telco_churn %>%
  mutate(probability_churn_glm = 
           predict(telco_churn_glm_train, newdata = telco_churn, type = "response")) %>%
  mutate(churn_numeric = Churn == "Yes") %>%
  filter(!is_train) %>%
  arrange(desc(probability_churn_glm)) %>%
  mutate(tpr=cumsum(churn_numeric)/sum(churn_numeric),
         fpr=cumsum(!churn_numeric)/sum(!churn_numeric)) %>%
  mutate(model = "logistic regression")

roc_prep <- telco_churn_roc %>%
  bind_rows(telco_churn_roc_deep,
            telco_churn_roc_glm)
  

ggplot(roc_prep, aes(x = fpr, y = tpr, color = model)) + 
  geom_line() + 
  xlab("False positive rate (1 - Specificity)") + 
  ylab("True positive rate (Sensitivity)") + 
  scale_x_continuous(labels = scales::percent) + 
  scale_y_continuous(labels = scales::percent) +
  ggtitle("An ROC for our churn decision tree model") +
  geom_abline(intercept = 0, slope = 1)


```
   
### לסיכום

   * עצי החלטה קל להסביר למקבלי החלטות
   * יכולים לבטא קשרים שאינם ניתנים לביטוי בנוסחת רגרסיה
   * ניתן בקלות להשתמש במשתנים קטגוריאליים, וגם להתמודד עם ערכים חסרים

אבל
   
   * כמודל חיזוי - האם לא כל כך טובים
   * הם לא רובוסטיים (שינוי קל בנתונים עלול להביא לשינוי מאוד מהותי במבנה העץ)
   
לכן, נבנו מספר אלגוריתמים המבוססים על עצים אך הם יותר רובוסטים ולרוב בעלי ביצועים טובים יותר.

   * יערות אקראיים - randomForests
   * Bagging
   * Boosting

## יערות אקראיים (randomForests)

האלגוריתם של יערות אקראיים בונה מקבץ של של עצים רבים, כאשר בכל עץ בכל פיצול, הוא מגביל את המשתנים לפיהם הוא יכול לפצל ל-$m$ משתנים בלבד (מתוך $p$ אפשריים). בדרך כלל $m\approx \sqrt{p}$).

כמו כן, האלגוריתם מגריל תצפיות (במקום להשתמש בכל התצפיות הוא משתמש במדגם שלהן), לצורך בנייתו של עץ.

אלגוריתם זה יכול לצמצם את ההשפעות של קורלציה בין משתנים, וכמו כן, הוא נותן הזדמנות למשתנים מסבירים שונים לבוא לידי ביטוי, אפילו אם הם לא בעלי העוצמה החזקה ביותר.

לבסוף התוצר המתקבל הוא ממוצע החיזויים על פני כלל העצים.

ערך מוסף בעצים הוא שניתן לחשב את ההפחתה הממוצעת במדד Gini של כל אחד מהמשתנים המסבירים, וזה מאפשר לדרג אותם לפי סדר חשיבות. בבעיית רגרסיה, החשיבות מסודרת לפי מידת "הטהורות" (במונחי RSS) שהוספת משתנה מסוים תרמה לדיוק, בממוצע.

```{r diamond random forest}

library(randomForest)

# note the use of maxnodes, otherwise the trees are grown to maximal size
# also limiting the number of trees to 150 - the default is 500...
diamond_price_forest <- randomForest(
  formula = price ~ .,
  data = diamonds,
  maxnodes = 15,
  ntree = 150)

# plot the importance plot
varImpPlot(diamond_price_forest)

# show an example of the first tree
getTree(diamond_price_forest, k = 1)


```

### תרגיל

   1. בנו יער אקראי לנתוני הנטישה של telco.
   2. חשבו על ה-test set את שיעור הטעות מסוג ראשון ושיעור הטעות מסוג שני.
   3. הוסיפו את הנתונים של יער זה לעקומות ה-ROC מהסעיף הקודם.
      a. שימו לב, כאשר אתם משתמשים בפונקציית predict, עליכם להגדיר את הפרמטר type בצורה מסויימת. איך?
   4. בנו תרשים של חשיבות המשתנים. מה המשתנה/ים החשוב/ים ביותר בהשפעה על נטישת/נאמנות לקוחות?
   5. מנכ"ל החברה מתלבט האם להציע הנחה ללקוחות אשר המודל צופה שינטשו. סמנכ"ל הכספים טוען שחבל להציע הנחה, משתנה זה אינו משמעותי מספיק בשביל שהנחה במחיר תצליח לגרום ללקוחות להישאר. מאידך סמנכ"ל שירות הלקוחות טוען שהנחה תעזור מאוד. מנכ"ל החברה ביקש מכם לשפוט - מי מהם צודק? הציעו מודל שיבחן את ההשפעה של הנחה ללקוחות מסוימים כדי לצמצם את הנטישה. האם כדי לנקוט בטקטיקת מתן הנחות לצורך גידול בפדיון? השתמשו במשתנה monthlycharges כדי לקבוע את שיעור ההנחה (אחוז מתוך משתנה זה, ללקוחות הרלוונטיים).

```
# some help: 
# first if you try to replicate the code from the diamond's example, you will get an error.
# this is because randomForest expects no character variables, just numeric and factors.
# So how can we turn everything character into factor?
# Intuitively, you would probably do:

# telco_churn <- telco_churn %>%
#    mutate(gender = as.factor(gender),
#           SeniorCitizen = as.factor(SeniorCitizen),
#           ...)

# but this is like doing the same action over and over again.
# wouldn't it be nice to just loop over everything, and if a column is of the wrong type (character)
# just convert it into factor?
# mutate_if() does exactly that.
# it needs a condition called .predicate, and a vector function to operate called .funs. 
# It goes like this:

telco_churn <- telco_churn %>%
   mutate_if(.predicate = funs(typeof(.)=="character"), 
             .funs = funs(as.factor(.)))

# What does it do?
# it's like looping
# for (i in 1:NCOL(telco_churn)){
#    if (typeof(telco_churn[,i]) == "character") {
#       telco_churn[,i] <- as.factor(telco_churn[,i])
#    }
# }

# the syntax of 
# typeof(.) == "character"
# is like "replace . with the vector you are currently checking"
# the syntax of as.factor(.) is like
# "replace . with the vector" that is a character and you need to type case into a factor
# the funs() function makes the expression explicit after replacing the . with the current vector

# now you can continue the exercise...
```
   
```{r example for random forest, include = FALSE}
library(randomForest)

# the randomForest needs preprocessing of characters into factors
telco_churn <- telco_churn %>%
  mutate_if(.predicate = funs(typeof(.) == "character") , funs(as.factor(.)))

# now build the forest, might take a while
churn_forest <- randomForest(formula = factor(Churn) ~ . - is_train,
                             data = telco_churn %>% filter(is_train),
                             importance = TRUE,
                             ntree = 500,
                             na.action = na.omit)

# show the first three trees
head(getTree(churn_forest, k = 1, labelVar = TRUE), 100)
head(getTree(churn_forest, k = 2, labelVar = TRUE), 100)
head(getTree(churn_forest, k = 3, labelVar = TRUE), 100)

# compute the confusion matrix - it uses the classification (majority) rule
telco_churn_predicted <- telco_churn %>%
  mutate(forest_churn_pred = predict(churn_forest, newdata = telco_churn, type = "response")) %>%
  filter(!is_train) %>%
  count(Churn, forest_churn_pred) %>%
  filter(!is.na(forest_churn_pred)) %>%
  group_by(Churn) %>%
  mutate(prop = n/sum(n)) %>%
  select(-n) %>%
  spread(forest_churn_pred, prop)
# type-I error is 10.4%, type-II error is 48.4%.

# variable importance chart
varImpPlot(churn_forest)

# compute prediction and ROC over test set
telco_churn_roc_forest <- telco_churn %>%
  mutate(probability_churn_forest = 
           predict(churn_forest, newdata = telco_churn, type = "prob")[,2]) %>%
  mutate(churn_numeric = Churn == "Yes") %>%
  filter(!is_train) %>%
  arrange(desc(probability_churn_forest)) %>%
  mutate(tpr=cumsum(churn_numeric)/sum(churn_numeric),
         fpr=cumsum(!churn_numeric)/sum(!churn_numeric)) %>%
  mutate(model = "random forest")

roc_prep <- telco_churn_roc %>%
  bind_rows(telco_churn_roc_deep,
            telco_churn_roc_glm,
            telco_churn_roc_forest)

ggplot(roc_prep, aes(x = fpr, y = tpr, color = model)) + 
  geom_line() + 
  xlab("False positive rate (1 - Specificity)") + 
  ylab("True positive rate (Sensitivity)") + 
  scale_x_continuous(labels = scales::percent) + 
  scale_y_continuous(labels = scales::percent) +
  ggtitle("An ROC for our churn detection models") +
  geom_abline(intercept = 0, slope = 1)


# q5 of the question - offering a discount.

# build a new classifier
# omit the TotalCharges variable (tenure and MonthlyCharges already express the total charges)
churn_detection <- glm(telco_churn %>% filter(!is_train),
                       formula = Churn ~ . - is_train -TotalCharges,
                       family = binomial)

# add the predicted variable
telco_discount <- telco_churn %>%
  mutate(predicted_churn = predict(churn_detection, newdata = telco_churn, type = "response")) %>%
  filter(!is_train)

# nominal revenue for the next month:
telco_discount %>% 
  mutate(rev_next_month = ifelse(Churn == "No", MonthlyCharges, (1-predicted_churn)*MonthlyCharges)) %>%
  summarize(sum(rev_next_month))

# build a function based on revenue for the next month and activation of discount rate, for the 
# observations about the discount quantile

calculate_predicted_rev <- function(discount_rate = 0.15, discount_quantile = 0.75){
  pred_quantile <- as.numeric(quantile(telco_discount$predicted_churn, discount_quantile))

  telco_discount_comp <- telco_discount %>% 
    mutate(give_discount = predicted_churn >= pred_quantile) %>%
    mutate(originalMonthlyCharges = MonthlyCharges) %>%
    mutate(MonthlyCharges = 
             ifelse(give_discount, originalMonthlyCharges*(1-discount_rate), originalMonthlyCharges)) %>%
    mutate(rev_next_month = ifelse(Churn == "No", MonthlyCharges, (1-predicted_churn)*MonthlyCharges)) %>%
    summarize(rev = sum(rev_next_month))
  
  return(telco_discount_comp$rev)
}

# now, create a grid with the function running on different combinations
# I'm going to use pmap to generate the results via functional programming,
# but this can also be accomplished by looping over lines of tibble
discount_options <- as.tibble(
  expand.grid(discount_rate = seq(0, 0.2, 0.025), 
              discount_quantile = seq(0.5, 0.95, 0.05))) %>%
  mutate(rev = map2_dbl(discount_rate, discount_quantile, `calculate_predicted_rev`))

ggplot(discount_options, aes(x = discount_rate, y = discount_quantile, fill = rev)) + 
  geom_raster(interpolate = TRUE) + 
  geom_contour(aes(z = rev), color = "yellow", size = 1.5) + 
  ggtitle("The expected revenue as a function of discount rate and discount quantile\n(above which churn likelihood quantile to apply it)")
  
```

## גישת ה-Boosting

ביערות אקראיים ראינו כיצד חזאי בודד (עץ) משתכפל והופך לשילוב של הרבה חזאים. כאשר מבוצע שילוב של חזאים רבים הדבר מהווה פוטנציאל להפחית שגיאות שעשויות להופיע באופן מקומי בחזאים אשר נתונים פעמים רבות ל"גחמות הסטטיסטיקה" (או שגיאות הנובעות ממינימום מקומי או מהתאמת יתר).

גישה נוספת חוץ מיערות אקראיים היא גישת ה-Boosting. היא יכולה להתאים לגישות שונות (לאו דווקא להכללה של עצים), אך פה נדגים אותה בהקשר העצים.

נניח שהבעיה שלנו היא בעיית רגרסיה (חיזוי ערך של משתנה רציף). ב-Boosting, בכל שלב האלגוריתם יבנה עץ, שהמטרה שלו היא חיזוי השגיאה (לא הערך האמיתי של $y$ אלא השגיאה הצפוייה בהתבסס על כל העצים שנבנו עד כה).

המודל מתווסף כסכום לכל יתר המודל שחושבו עד כה, עם פרמטר "הקטנה" $\lambda$.

במילים אחרות, האלגוריתם מרכיב סכום של הרבה עצים קטנים, כשכל פעם הוא נותן דגש על צמצום השגיאות שהתקבלו עד כה.

```
Pseudo code:
Set r = y, f(x)=0
For b=1, 2,..., B repeat:
   Fit a tree, f_curr, to the data (X,r)
   Update f by adding current learned tree: 
      f <- f + lambda*f_curr
   Update the residuals
      r - r-lambda*f_curr
Output f
```

במקרה של בעיות סיווג, העדכון של המודלים מתבצע על ידי בניה בכל שלב של מודל סיווג (לדוגמה עץ), תוך כדי מתן משקל גדול יותר לתצפיות אשר הסיווג שלהן שגוי.

ב-R יש שתי חבילות המשמשות לboosting:

   * adabag
   * xgboost

### תרגיל

השתמש בחבילת adabag, בפקודת `boosting` כדי לייצר חזאי לנטישת לקוחות.
הצגת התוצאות בתרשים ה-ROC. האם ביצוע boosting שיפר את החזאים?

```{r boosting example using adabag}
library(adabag)

churn_boost <- adabag::boosting(formula = Churn ~ . - is_train,
                                data = telco_churn %>% filter(is_train),
                                control = rpart.control(cp = 0.1),
                                mfinal = 50)

```